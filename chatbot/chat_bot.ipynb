{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af2c2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_core.messages import AIMessage\n",
    "from langgraph.graph import START, MessagesState, StateGraph\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95ec0c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "name = \"Sami\"\n",
    "model_name = os.getenv(\"OLLAMA_MODEL\")\n",
    "model = ChatOllama(\n",
    "    model=model_name,\n",
    "     temperature=0.3,\n",
    "     max_tokens=1000,\n",
    "     )\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    messages = [\n",
    "         SystemMessage(\n",
    "            content=\"You are an all purpose helpful assistant that can answer questions, provide explanations, and assist with various tasks.\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content= f\"My name is {name}.\"\n",
    "        ),\n",
    "       \n",
    "        AIMessage(\n",
    "            content=f\"Hello! How can I assist you today {name}?\"\n",
    "        ),\n",
    "        HumanMessage(\n",
    "            content=\"How old is Earth?\"),\n",
    "            \n",
    "        HumanMessage(\n",
    "            content= \"Whats my name again?\"\n",
    "        ),\n",
    "    ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b79e1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    response = model.invoke(state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "workflow = StateGraph(state_schema=MessagesState)\n",
    "workflow.add_node(\"model\", call_model)\n",
    "workflow.set_entry_point(\"model\")\n",
    "\n",
    "# Add memory\n",
    "memory = InMemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "80add5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, the user asked two questions. First, how old the Earth is, and second, what their name is again. Let me start with the first question.\n",
      "\n",
      "I know the Earth's age is a well-established scientific fact. The commonly accepted estimate is about 4.54 billion years. I should mention that this is based on radiometric dating of the oldest rocks and meteorites. It's important to note that this figure is supported by multiple lines of evidence, so I should explain that to give context.\n",
      "\n",
      "Now, the second question. The user previously mentioned their name is Sami, so I should confirm that. But maybe they just want to make sure I remember. I should respond with \"Sami\" and maybe add a friendly note to confirm that I remember correctly. It's a simple check, but it's good to be accurate.\n",
      "\n",
      "I need to make sure the answers are clear and concise. For the Earth's age, provide the number and the method used. For the name, just state it and maybe a brief acknowledgment. Keep the tone friendly and helpful. No need for complex terms, just straightforward information. Also, check if there's any follow-up they might need, but since they asked two separate questions, just answer them directly.\n",
      "</think>\n",
      "\n",
      "The Earth is approximately **4.54 billion years old**, based on radiometric dating of the oldest rocks and meteorites. \n",
      "\n",
      "Your name is **Sami**! ðŸ˜Š Let me know if you need help with anything else.\n"
     ]
    }
   ],
   "source": [
    "model.invoke(messages)\n",
    "print(model.invoke(messages).content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd84a81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sennaciri-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
